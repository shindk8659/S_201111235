{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.6')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#이거꼭해주기\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).config('spark.sql.warehouse.dir','file:///C:/Users/DongkyuShin/Desktop/bigdata/s_201111235/201111235/src').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\n",
      "C:\\Users\\DongkyuShin\\AppData\\Local\\Temp\\spark-779fd304-251f-4ab4-811f-0bd8da8aa06e\\userFiles-5f9d0d89-5697-4ab9-961a-409c865914c1\n",
      "C:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\n",
      "\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\python27.zip\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\DLLs\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\plat-win\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\lib-tk\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\Sphinx-1.5.1-py2.7.egg\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\win32\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\setuptools-27.2.0-py2.7.egg\n",
      "C:\\Users\\DongkyuShin\\Anaconda2\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\DongkyuShin\\.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "172.16.24.243\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.get.\n: java.util.NoSuchElementException: spark.jars.packages\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:68)\r\n\tat sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-44f28182ed91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.jars.packages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\conf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.get.\n: java.util.NoSuchElementException: spark.jars.packages\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:68)\r\n\tat sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile data\\ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(value=u'Wikipedia')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = myRdd3.map(lambda line: line.split(','))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 4 1.11803398875 1.25\n"
     ]
    }
   ],
   "source": [
    "print nRdd.sum(), nRdd.min(), nRdd.max(), nRdd.stdev(), nRdd.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=myRdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=myRdd2.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in words.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  espresso|\n",
      "| lee|     latte|\n",
      "| lim| americano|\n",
      "| kim|  affocato|\n",
      "| lee|long black|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%5]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)\n",
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.78  71.52  69.4   68.22  67.79]\n",
      "[ 112.99  136.49  153.03  142.34  144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x86e50b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)\n",
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=spark.createDataFrame(wc)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DongkyuShin\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(1)\n",
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "wcDF=spark.createDataFrame(wcRdd)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992-12-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"12/12/1992\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, DoB=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=wcDF.withColumn('DoB', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \",row1.year, row1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation')\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kim', u' js']\n"
     ]
    }
   ],
   "source": [
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()\n",
    "print rd.values()[0].split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n",
      "+----+-----------+\n",
      "|year|max(height)|\n",
      "+----+-----------+\n",
      "|   1|        175|\n",
      "|   2|        180|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()\n",
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))\n",
    "print int('1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "toint=udf(lambda x:int(x),IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\",toint(myDf['year']))\n",
    "\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DongkyuShin\\Desktop\\bigdata\\s_201111235\\201111235\\data\\kddcup.data_10_percent.gz data does not exist! retrieving..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'normal.' in x)\n",
    "print _normal.count()\n",
    "_csvRdd=_rdd.map(lambda x: x.split(','))\n",
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kv = _csvRdd.map(lambda x: (x[41], 1))\n",
    "_attack = _kv.reduceByKey(lambda x,y: x+y)\n",
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_csvRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
